{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=8, suppress=True, linewidth=400, threshold=100)\n",
    "import gym\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorimotorAutoencoderAgents(object):\n",
    "    '''\n",
    "    a group of autoencoders, each with the ability to encode one transition\n",
    "    that work together to form a predictive sensorimotor inference engine.\n",
    "    basically they map the space, distributedly, so that they can find a path\n",
    "    from any observation to any other observation - they know how to manipulate\n",
    "    the environment.\n",
    "\n",
    "    they have overlapping input bits, but no two have the same inputs. some\n",
    "    have no inputs from the environment at all, and instead get inputs only\n",
    "    from other autoencoders. There are typically many autoencoders. they\n",
    "    automatically wire themselves up (inefficienty, but successfully).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, env, encoders_n=12):\n",
    "        self.env = env\n",
    "        self.encoders = self.generate_encoders(encoders_n)\n",
    "\n",
    "    def generate_encoders(self, n):\n",
    "        ''' https://blog.keras.io/building-autoencoders-in-keras.html '''\n",
    "        from keras.layers import Input, Dense\n",
    "        from keras.models import Model\n",
    "        encoders = []\n",
    "        for i in range(n):\n",
    "            # this is the size of our encoded representations\n",
    "            encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "            # this is our input placeholder\n",
    "            input_img = Input(shape=(784,))\n",
    "            # \"encoded\" is the encoded representation of the input\n",
    "            encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "            # \"decoded\" is the lossy reconstruction of the input\n",
    "            decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "            # this model maps an input to its reconstruction\n",
    "            autoencoder = Model(input_img, decoded)\n",
    "            # Let's also create a separate encoder model:\n",
    "            # this model maps an input to its encoded representation\n",
    "            encoder = Model(input_img, encoded)\n",
    "            # As well as the decoder model:\n",
    "            # create a placeholder for an encoded (32-dimensional) input\n",
    "            encoded_input = Input(shape=(encoding_dim,))\n",
    "            # retrieve the last layer of the autoencoder model\n",
    "            # Here we need to change this:\n",
    "            # we want the decoder_layer to be the next timestep so we can train the\n",
    "            # autoencoder on the transition from\n",
    "            # one observation+action to a new observation:\n",
    "            decoder_layer = autoencoder.layers[-1]\n",
    "            # create the decoder model\n",
    "            decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "            # autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "            encoders.append(autoencoder)\n",
    "        # now wire them up up so they share latents to each other's inputs (at random)\n",
    "        # also wire them up at random to the environment, and the action space...\n",
    "        return encoders\n",
    "\n",
    "    def step(self, obs):\n",
    "        # they are predicting what action they will take. at first the observation\n",
    "        # stands in as a random seed to activate the network, but soon they\n",
    "        # wire up in a hierarchy and take actions to acheive what they think\n",
    "        # they will see, instead of providing goals, you provide an image of\n",
    "        # what you want them to see at the top layer of the hierarchy...\n",
    "        sampled = env.action_space.sample()\n",
    "        print(f'action sampled: {sampled}')\n",
    "        return sampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCube(gym.Env):\n",
    "    ''' a RubixCube with only two colors. so that every face can be binary '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def __init__(self):\n",
    "        super(RubixCube, self).__init__()\n",
    "        self.action_space = self._action_space()\n",
    "        self.observation_space = self._observation_space()\n",
    "        # should change the state to be a list of np array?\n",
    "        self.cube_state =[\n",
    "            1, 1, 1, 1,\n",
    "            1, 1, 1, 1,\n",
    "            0,\n",
    "            0, 0, 0,\n",
    "            1, 1, 1,\n",
    "            1, 1, 1,\n",
    "            0, 0, 0,\n",
    "            0, 0,\n",
    "            1, 1,\n",
    "            1, 1,\n",
    "            0, 0,\n",
    "            0, 0, 0,\n",
    "            1, 1, 1,\n",
    "            1, 1, 1,\n",
    "            0, 0,\n",
    "            0, 0, 0, 0,\n",
    "            0, 0, 0, 0]\n",
    "        self.solved_state = copy.deepcopy(self.cube_state)\n",
    "        self.do_right = {\n",
    "            3: 16, 16: 45, 45: 32, 32: 3,\n",
    "            4: 26, 26: 44, 44: 23, 23: 4,\n",
    "            5: 36, 36: 43, 43: 12, 12: 5,\n",
    "            14: 25, 25: 34, 34: 24, 24: 14,\n",
    "            35: 33, 33: 13, 13: 15, 15: 35, }\n",
    "        self.do_left = {\n",
    "            7: 10, 10: 41, 41: 38, 38: 7,\n",
    "            8: 22, 22: 48, 48: 27, 27: 8,\n",
    "            1: 30, 30: 47, 47: 18, 18: 1,\n",
    "            19: 9, 9: 29, 29: 39, 39: 19,\n",
    "            20: 21, 21: 40, 40: 28, 28: 20, }\n",
    "        self.do_top = {\n",
    "            9: 12, 12: 15, 15: 18, 18: 9,\n",
    "            10: 13, 13: 16, 16: 19, 19: 10,\n",
    "            11: 14, 14: 17, 17: 20, 20: 11,\n",
    "            1: 3, 3: 5, 5: 7, 7: 1,\n",
    "            2: 4, 4: 6, 6: 8, 8: 2, }\n",
    "        self.do_under = {\n",
    "            30: 33, 33: 36, 36: 39, 39: 30,\n",
    "            31: 34, 34: 37, 37: 40, 40: 31,\n",
    "            32: 35, 35: 38, 38: 29, 29: 32,\n",
    "            41: 43, 43: 45, 45: 47, 47: 41,\n",
    "            42: 44, 44: 46, 46: 48, 48: 42, }\n",
    "        self.do_front = {\n",
    "            1: 13, 13: 43, 43: 29, 29: 1,\n",
    "            2: 24, 24: 42, 42: 21, 21: 2,\n",
    "            3: 33, 33: 41, 41: 9, 9: 3,\n",
    "            10: 12, 12: 32, 32: 30, 30: 10,\n",
    "            11: 23, 23: 31, 31: 22, 22: 11, }\n",
    "        self.do_back = {\n",
    "            7: 15, 15: 45, 45: 39, 39: 7,\n",
    "            6: 25, 25: 46, 46: 28, 28: 6,\n",
    "            5: 35, 35: 47, 47: 19, 19: 5,\n",
    "            18: 16, 16: 36, 36: 38, 38: 18,\n",
    "            17: 26, 26: 37, 37: 27, 27: 17, }\n",
    "    def step(self, action):\n",
    "        return self._request(action)\n",
    "    def reset(self):\n",
    "        return self._request(None)[0]\n",
    "    def render(self, mode='human', close=False):\n",
    "        action, obs, reward, done, info = self.state\n",
    "        if action == None: print(\"{}\\n\".format(obs))\n",
    "        else: print(\"{}\\t\\t--> {:.18f}{}\\n{}\\n\".format(action, reward, (' DONE!' if done else ''), obs))\n",
    "\n",
    "    def _action_space(self):\n",
    "        '''\n",
    "        left, right, top, under, front, back\n",
    "        this is a deterministic env, it doesn't change unless you change it,\n",
    "        therefore, no opperation isn't available.\n",
    "        '''\n",
    "        return gym.spaces.Discrete(6)\n",
    "\n",
    "    def _observation_space(self):\n",
    "        return gym.spaces.Box(low=np.NINF, high=np.inf, shape=(48,), dtype=np.float64)\n",
    "\n",
    "    def _request(self, action):\n",
    "        cube = copy.deepcopy(self.cube_state)\n",
    "        if isinstance(action, int):\n",
    "            action = {\n",
    "                0: 'left', 1: 'right',\n",
    "                2: 'top', 3: 'under',\n",
    "                4: 'front', 5: 'back'}.get(action, None)\n",
    "        if action is not None:\n",
    "            for k, v in eval(f'self.do_{action}').items():\n",
    "                self.cube_state[k] = cube[v]\n",
    "        obs = self.cube_state\n",
    "        reward = np.float64(0.0)  # real AGI doesn't need spoonfed 'rewards'\n",
    "        done = False\n",
    "        info = {}\n",
    "        self.state = (action, obs, reward, done, info)\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleCube()\n",
    "env.seed(0)\n",
    "print(\"agent: env.action_space {}\".format(env.action_space))\n",
    "agent = SensorimotorAutoencoderAgents(env)\n",
    "for i_episode in range(1):\n",
    "    obs = env.reset()\n",
    "    env.render()\n",
    "    for t_timesteps in range(1000):\n",
    "        action = agent.step(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
